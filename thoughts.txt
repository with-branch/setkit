Allow mapping over dataset views. Apply the changes to only the indices within the view.

Essentially there are three types of datasets
GeneratorDataset
	Multithreadable
	Does not need indices
	Does not need a prepare_data, instead needs a nextitem
	Cannot map
	Should be able to be read into a CollectionDataset (maybe by using slices?)
IterableDataset
	Not multithreadable
	Should have a prepare data, which returns an iterator
	Use when you cannot load all of the iterator into memory, large file
	Does it need a download?
	Only needs getitem
	Cannot use setitem
	Cannot map
	Can use transforms
	Should be able to be read into a CollectionDataset (maybe by using slices?)
CollectionDataset
	Multithreadable
	Should have getitem and setitem
	Implements map, using getitem and setitem
	
	FileDataset
		Does it need a prepare_data? (Not if it's an unsup dataset)
			What should prepare data return? A special data item?
		How does the user connect the labels?
		Must implement a getitem
		
If an iterable dataset returns an iterator, how is that
different than prepare_data for a collection dataset? They
are pretty related, no?

How do you map over a FileDataset?
	Should we just have a StreamDataset?
	This would be for both files and urls and uris etc.
	Basically it would house a location, and the user
	would implement a way of loading from that uri.

RootflowDataset
	IndexableDataset, torch:Datasets
		StreamDataset
		MappableDataset
			CollectionDataset
			CollectionDatasetView
			CollectionDatasetConcat
	NonIndexableDataset, torch:IterableDataset
		IterableDataset
		GeneratorDataset
			Make sure that this guy multithreads properly
	

What are the iterable dataset use cases?
Streaming data -> Generator or Collection depending
Synthetic data -> GeneratorDataset
Reading a file (Think lines of text) -> IterableDataset
	Because you read the file sequentially, you cannot
	parralellize the data.

What about saving a dataset? And loading a saved dataset?


GeneratorDataset:
	download -> None
X	yeild_item -> DataItem
No item ids?
	
StreamDataset:
	download -> None
X	prepare_data -> List[address]
X	fetch_item -> DataItem
	
CollectionDataset:
	download -> None
X	prepare_data -> List[DataItem]
	get_item -> DataItem
	DataItem -> set_item
	
IterableDataset:
	download -> None
X	prepare_data -> iterator[DataItem]

-----------------------------------------------
-----------------------------------------------

Open Lyrics
	How do we collect k number of files for n data items, when k < n?
	
Dataset
	item_paths
	prepare_data (if has, then load in memory) (allows List[Any], converts Any to data of List[DataItem])
	next_item
	get_item
	set_item
	map (only works if we have prepare_data)

----------------------------------------------
----------------------------------------------
rootflow-datasets:

If `prepare_data` uses yeild, the syntax becomes easier for the user
Also, then I control the loading loop, and can place tqdm in there

Using a zarr is about 6.4x faster. Is it worth it to cache zarrs by default?
This would basically mean that using a rootflow dataset would double your
disk usage, which is not very good.

If we add metadata, does that mean we need to map over it?

Datasets I want to support:
	Open Lyrics
		~25.2 GB on local disk
		Option to load into RAM or load from disk
		Known number of items (could load in chunks)
	Crypto Regulations Dataset
		JSON files in Drive.
		Probably too large to download at some point
		Known number of items
	Labeled Email Dataset
		14GB
		Zarr file
		Needs metadata for filtering
	Hatespace Ironmarch Dataset
		Small ~
	Hatecomp Datasets
	Convolutional Salesman Dataset
		Can be generated on the fly
		Should support pregenerating a large number (somehow generate and cache? Then we serve new items as fast as we are able, while still keeping up with the model?)
	Memes Dataset
	Minecraft Dataset
	Audio Sample Comparison Dataset
	Synth Food Dataset
